{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxGUO4VzHUTt/IcdUpkWdx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAw1O6okvWO-",
        "outputId": "3cc7214f-6070-4064-a66a-d337ee88ced6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success, tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Discriminator and Generator implementation from DCGAN paper\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib.colors import Normalize\n",
        "from spacy.kb.kb_in_memory import Writer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.transforms.v2 import Transform\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input = N x img_channels x 64 x 64\n",
        "            nn.Conv2d(img_channels, features_d, kernel_size=4, stride=2, padding=1), # 32 x 32\n",
        "            nn.LeakyReLU(0.2),\n",
        "            self._block(features_d, features_d*2, 4, 2, 1), # 16 x 16\n",
        "            self._block(features_d*2, features_d * 4, 4, 2, 1), # 8 x 8\n",
        "            self._block(features_d*4, features_d * 8, 4, 2, 1), # 4 x 4\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1 x 1\n",
        "            nn.Sigmoid(),\n",
        "\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "    print(\"Success, tests passed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of DCGAN network on MNIST dataset with discriminator and generator imported from model.py\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "LEARNING_RATE=2e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_GEN = 64\n",
        "FEATURES_DISC = 64\n",
        "Z_DIM = 100\n",
        "IMAGE_SIZE = 64\n",
        "IMG_CHANNELS = 1\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize # Import necessary transforms\n",
        "transform_pipeline = Compose(\n",
        "        [\n",
        "             Resize(IMAGE_SIZE),\n",
        "            ToTensor(),\n",
        "            Normalize((0.5), (0.5))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "dataset = datasets.MNIST(root='dataset/', train=True, transform=transform_pipeline, download=True)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(Z_DIM, IMG_CHANNELS, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(IMG_CHANNELS, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "Writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "Writer_real = SummaryWriter(f\"logs/real\")\n",
        "step = 0\n",
        "criterion = nn.BCELoss()\n",
        "gen.train()\n",
        "disc.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_idx, (real, _) in enumerate(loader):\n",
        "        real = real.to(device)\n",
        "        ### we need to maximize discriminator loss which is log(D(real)) + log(1 - D(G(z)))\n",
        "\n",
        "        ### where z is the random noise to be given as input to the generator to generate fake image\n",
        "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "        disc_real = disc(real).view(-1)  # D(real)\n",
        "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))  # log(D(real))\n",
        "\n",
        "        disc_fake = disc(fake.detach()).view(-1)  # D(G(z))\n",
        "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))  # log(1 - D(G(z)))\n",
        "\n",
        "        lossD = (lossD_real + lossD_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        lossD.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        output = disc(fake).view(-1)\n",
        "        lossG = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        lossG.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)}  Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                Writer_real.add_image(\"real\", img_grid_real, global_step=step)\n",
        "                Writer_fake.add_image(\"fake\", img_grid_real, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GA1OV8dvcav",
        "outputId": "eeacbf63-f0b1-4da5-e291-042c75c7e2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5] Batch 0/469  Loss D: 0.6864, Loss G: 0.7920\n",
            "Epoch [0/5] Batch 100/469  Loss D: 0.0148, Loss G: 4.1165\n",
            "Epoch [0/5] Batch 200/469  Loss D: 0.7340, Loss G: 0.7049\n",
            "Epoch [0/5] Batch 300/469  Loss D: 0.5275, Loss G: 1.0397\n",
            "Epoch [0/5] Batch 400/469  Loss D: 0.5978, Loss G: 1.1456\n",
            "Epoch [1/5] Batch 0/469  Loss D: 0.5944, Loss G: 0.8268\n",
            "Epoch [1/5] Batch 100/469  Loss D: 0.6066, Loss G: 0.7971\n",
            "Epoch [1/5] Batch 200/469  Loss D: 0.6361, Loss G: 0.8992\n",
            "Epoch [1/5] Batch 300/469  Loss D: 0.6966, Loss G: 0.7966\n",
            "Epoch [1/5] Batch 400/469  Loss D: 0.6705, Loss G: 0.7006\n",
            "Epoch [2/5] Batch 0/469  Loss D: 0.5973, Loss G: 0.8740\n",
            "Epoch [2/5] Batch 100/469  Loss D: 0.5943, Loss G: 0.8484\n",
            "Epoch [2/5] Batch 200/469  Loss D: 0.6465, Loss G: 0.8421\n",
            "Epoch [2/5] Batch 300/469  Loss D: 0.5943, Loss G: 1.0008\n",
            "Epoch [2/5] Batch 400/469  Loss D: 0.8369, Loss G: 1.0922\n",
            "Epoch [3/5] Batch 0/469  Loss D: 0.5717, Loss G: 1.3017\n",
            "Epoch [3/5] Batch 100/469  Loss D: 0.5993, Loss G: 1.7477\n",
            "Epoch [3/5] Batch 200/469  Loss D: 0.4521, Loss G: 1.7272\n",
            "Epoch [3/5] Batch 300/469  Loss D: 0.4811, Loss G: 2.0422\n",
            "Epoch [3/5] Batch 400/469  Loss D: 0.3743, Loss G: 2.7035\n",
            "Epoch [4/5] Batch 0/469  Loss D: 0.4288, Loss G: 2.0996\n",
            "Epoch [4/5] Batch 100/469  Loss D: 0.2520, Loss G: 2.0132\n",
            "Epoch [4/5] Batch 200/469  Loss D: 0.5823, Loss G: 2.7260\n",
            "Epoch [4/5] Batch 300/469  Loss D: 0.2630, Loss G: 1.5675\n",
            "Epoch [4/5] Batch 400/469  Loss D: 0.4728, Loss G: 3.5671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1OK6Bc9vyJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}